{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c0e21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650a2a1d5a6a427190c473592bd6f0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import token\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_key = os.getenv(\"HUGGING_FACE_KEY\")\n",
    "\n",
    "login(token=hf_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed176a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'gaia-benchmark/GAIA' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'gaia-benchmark/GAIA' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027441af86d5403eb3695c9e037058b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset 'gaia-benchmark/GAIA' is a gated dataset on the Hub. You must be authenticated to access it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3929997200.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m dataset = load_dataset(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"gaia-benchmark/GAIA\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"2023_all\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   1393\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fix_for_backward_compatible_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1133\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m   1023\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't reach the Hugging Face Hub for dataset '{path}': {e1}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataFilesNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmptyDatasetError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m                     raise FileNotFoundError(\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m403\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m                     \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\" Visit the dataset page at https://huggingface.co/datasets/{path} to ask for access.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRevisionNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m                 raise DatasetNotFoundError(\n",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset 'gaia-benchmark/GAIA' is a gated dataset on the Hub. You must be authenticated to access it."
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"gaia-benchmark/GAIA\",\n",
    "    \"2023_all\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "\n",
    "test_data = dataset[\"test\"]\n",
    "validation_data = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89ed407b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['test', 'validation'])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc0bb970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset sizes:\n",
      "Validation: 165 examples\n",
      "Test: 301 examples\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataset sizes:\")\n",
    "print(f\"Validation: {len(dataset['validation'])} examples\")\n",
    "print(f\"Test: {len(dataset['test'])} examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea4eceea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset sizes:\n",
      "Validation: 165 examples\n",
      "Test: 301 examples\n",
      "\n",
      "Columns in validation split:\n",
      "['task_id', 'Question', 'Level', 'Final answer', 'file_name', 'file_path', 'Annotator Metadata']\n",
      "\n",
      "Dataset features:\n",
      "{'task_id': Value(dtype='string', id=None), 'Question': Value(dtype='string', id=None), 'Level': Value(dtype='string', id=None), 'Final answer': Value(dtype='string', id=None), 'file_name': Value(dtype='string', id=None), 'file_path': Value(dtype='string', id=None), 'Annotator Metadata': {'Steps': Value(dtype='string', id=None), 'Number of steps': Value(dtype='string', id=None), 'How long did this take?': Value(dtype='string', id=None), 'Tools': Value(dtype='string', id=None), 'Number of tools': Value(dtype='string', id=None)}}\n"
     ]
    }
   ],
   "source": [
    "# 2. Check size of each split\n",
    "print(\"\\nDataset sizes:\")\n",
    "print(f\"Validation: {len(dataset['validation'])} examples\")\n",
    "print(f\"Test: {len(dataset['test'])} examples\")\n",
    "\n",
    "# 3. See the column names (fields)\n",
    "print(\"\\nColumns in validation split:\")\n",
    "print(dataset['validation'].column_names)\n",
    "\n",
    "# 4. See dataset features (data types)\n",
    "print(\"\\nDataset features:\")\n",
    "print(dataset['validation'].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93d17550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['task_id', 'Question', 'Level', 'Final answer', 'file_name', 'file_path', 'Annotator Metadata']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['validation'].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c63cf7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_id: c61d22de-5f6c-4958-a7f6-5e9707bd3466\n",
      "Question: A paper about AI regulation that was originally submitted to arXiv.org in June 2022 shows a figure with three axes, where each axis has a label word at both ends. Which of these words is used to describe a type of society in a Physics and Society article submitted to arXiv.org on August 11, 2016?\n",
      "Level: 2\n",
      "Final answer: egalitarian\n",
      "file_name: \n",
      "file_path: \n",
      "Annotator Metadata: {'Steps': '1. Go to arxiv.org and navigate to the Advanced Search page.\\n2. Enter \"AI regulation\" in the search box and select \"All fields\" from the dropdown.\\n3. Enter 2022-06-01 and 2022-07-01 into the date inputs, select \"Submission date (original)\", and submit the search.\\n4. Go through the search results to find the article that has a figure with three axes and labels on each end of the axes, titled \"Fairness in Agreement With European Values: An Interdisciplinary Perspective on AI Regulation\".\\n5. Note the six words used as labels: deontological, egalitarian, localized, standardized, utilitarian, and consequential.\\n6. Go back to arxiv.org\\n7. Find \"Physics and Society\" and go to the page for the \"Physics and Society\" category.\\n8. Note that the tag for this category is \"physics.soc-ph\".\\n9. Go to the Advanced Search page.\\n10. Enter \"physics.soc-ph\" in the search box and select \"All fields\" from the dropdown.\\n11. Enter 2016-08-11 and 2016-08-12 into the date inputs, select \"Submission date (original)\", and submit the search.\\n12. Search for instances of the six words in the results to find the paper titled \"Phase transition from egalitarian to hierarchical societies driven by competition between cognitive and social constraints\", indicating that \"egalitarian\" is the correct answer.', 'Number of steps': '12', 'How long did this take?': '8 minutes', 'Tools': '1. Web browser\\n2. Image recognition tools (to identify and parse a figure with three axes)', 'Number of tools': '2'}\n"
     ]
    }
   ],
   "source": [
    "example = dataset['validation'][0]\n",
    "\n",
    "for key, value in example.items():\n",
    "    if value is not None:\n",
    "        print(f\"{key}: {value}\")\n",
    "    else:\n",
    "        print(f\"{key}: None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7ff04a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset_builder\n",
    "\n",
    "ds_builder = load_dataset_builder(\"gaia-benchmark/GAIA\", \"2023_all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7e3e5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_builder.info.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21679560",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_validation_data = validation_data.filter(lambda example: example[\"file_name\"]== \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbde2e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd111731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_id': ['c61d22de-5f6c-4958-a7f6-5e9707bd3466',\n",
       "  '17b5a6a3-bc87-42e8-b0fb-6ab0781ef2cc',\n",
       "  '04a04a9b-226c-43fd-b319-d5e89743676f',\n",
       "  '14569e28-c88c-43e4-8c32-097d35b9a67d',\n",
       "  'e1fc63a2-da7a-432f-be78-7c4a95598703',\n",
       "  '8e867cd7-cff9-4e6c-867a-ff5ddc2550be',\n",
       "  '3627a8be-a77f-41bb-b807-7e1bd4c0ebdf',\n",
       "  '7619a514-5fa8-43ef-9143-83b66a43d7a4',\n",
       "  'ec09fa32-d03f-4bf8-84b0-1f16922c3ae4',\n",
       "  '676e5e31-a554-4acc-9286-b60d90a92d26'],\n",
       " 'Question': ['A paper about AI regulation that was originally submitted to arXiv.org in June 2022 shows a figure with three axes, where each axis has a label word at both ends. Which of these words is used to describe a type of society in a Physics and Society article submitted to arXiv.org on August 11, 2016?',\n",
       "  'I’m researching species that became invasive after people who kept them as pets released them. There’s a certain species of fish that was popularized as a pet by being the main character of the movie Finding Nemo. According to the USGS, where was this fish found as a nonnative species, before the year 2020? I need the answer formatted as the five-digit zip codes of the places the species was found, separated by commas if there is more than one place.',\n",
       "  'If we assume all articles published by Nature in 2020 (articles, only, not book reviews/columns, etc) relied on statistical significance to justify their findings and they on average came to a p-value of 0.04, how many papers would be incorrect as to their claims of statistical significance? Round the value up to the next integer.',\n",
       "  'In Unlambda, what exact charcter or text needs to be added to correct the following code to output \"For penguins\"? If what is needed is a character, answer with the name of the character. If there are different names for the character, use the shortest. The text location is not needed. Code:\\n\\n`r```````````.F.o.r. .p.e.n.g.u.i.n.si',\n",
       "  'If Eliud Kipchoge could maintain his record-making marathon pace indefinitely, how many thousand hours would it take him to run the distance between the Earth and the Moon its closest approach? Please use the minimum perigee value on the Wikipedia page for the Moon when carrying out your calculation. Round your result to the nearest 1000 hours and do not use any comma separators if necessary.',\n",
       "  'How many studio albums were published by Mercedes Sosa between 2000 and 2009 (included)? You can use the latest 2022 version of english wikipedia.',\n",
       "  \"The object in the British Museum's collection with a museum number of 2012,5015.17 is the shell of a particular mollusk species. According to the abstract of a research article published in Science Advances in 2021, beads made from the shells of this species were found that are at least how many thousands of years old?\",\n",
       "  'According to github, when was Regression added to the oldest closed numpy.polynomial issue that has the Regression label in MM/DD/YY?',\n",
       "  'Here\\'s a fun riddle that I think you\\'ll enjoy.\\n\\nYou have been selected to play the final round of the hit new game show \"Pick That Ping-Pong\". In this round, you will be competing for a large cash prize. Your job will be to pick one of several different numbered ping-pong balls, and then the game will commence. The host describes how the game works.\\n\\nA device consisting of a winding clear ramp and a series of pistons controls the outcome of the game. The ramp feeds balls onto a platform. The platform has room for three ping-pong balls at a time. The three balls on the platform are each aligned with one of three pistons. At each stage of the game, one of the three pistons will randomly fire, ejecting the ball it strikes. If the piston ejects the ball in the first position on the platform the balls in the second and third position on the platform each advance one space, and the next ball on the ramp advances to the third position. If the piston ejects the ball in the second position, the ball in the first position is released and rolls away, the ball in the third position advances two spaces to occupy the first position, and the next two balls on the ramp advance to occupy the second and third positions on the platform. If the piston ejects the ball in the third position, the ball in the first position is released and rolls away, the ball in the second position advances one space to occupy the first position, and the next two balls on the ramp advance to occupy the second and third positions on the platform.\\n\\nThe ramp begins with 100 numbered ping-pong balls, arranged in ascending order from 1 to 100. The host activates the machine and the first three balls, numbered 1, 2, and 3, advance to the platform. Before the random firing of the pistons begins, you are asked which of the 100 balls you would like to pick. If your pick is ejected by one of the pistons, you win the grand prize, $10,000.\\n\\nWhich ball should you choose to maximize your odds of winning the big prize? Please provide your answer as the number of the ball selected.',\n",
       "  'In July 2, 1959 United States standards for grades of processed fruits, vegetables, and certain other products listed as dehydrated, consider the items in the \"dried and dehydrated section\" specifically marked as dehydrated along with any items in the Frozen/Chilled section that contain the whole name of the item, but not if they\\'re marked Chilled. As of August 2023, what is the percentage (to the nearest percent) of those standards that have been superseded by a new version since the date given in the 1959 standards?'],\n",
       " 'Level': ['2', '2', '2', '2', '1', '1', '2', '2', '1', '3'],\n",
       " 'Final answer': ['egalitarian',\n",
       "  '34689',\n",
       "  '41',\n",
       "  'backtick',\n",
       "  '17',\n",
       "  '3',\n",
       "  '142',\n",
       "  '04/15/18',\n",
       "  '3',\n",
       "  '86'],\n",
       " 'file_name': ['', '', '', '', '', '', '', '', '', ''],\n",
       " 'file_path': ['', '', '', '', '', '', '', '', '', ''],\n",
       " 'Annotator Metadata': [{'Steps': '1. Go to arxiv.org and navigate to the Advanced Search page.\\n2. Enter \"AI regulation\" in the search box and select \"All fields\" from the dropdown.\\n3. Enter 2022-06-01 and 2022-07-01 into the date inputs, select \"Submission date (original)\", and submit the search.\\n4. Go through the search results to find the article that has a figure with three axes and labels on each end of the axes, titled \"Fairness in Agreement With European Values: An Interdisciplinary Perspective on AI Regulation\".\\n5. Note the six words used as labels: deontological, egalitarian, localized, standardized, utilitarian, and consequential.\\n6. Go back to arxiv.org\\n7. Find \"Physics and Society\" and go to the page for the \"Physics and Society\" category.\\n8. Note that the tag for this category is \"physics.soc-ph\".\\n9. Go to the Advanced Search page.\\n10. Enter \"physics.soc-ph\" in the search box and select \"All fields\" from the dropdown.\\n11. Enter 2016-08-11 and 2016-08-12 into the date inputs, select \"Submission date (original)\", and submit the search.\\n12. Search for instances of the six words in the results to find the paper titled \"Phase transition from egalitarian to hierarchical societies driven by competition between cognitive and social constraints\", indicating that \"egalitarian\" is the correct answer.',\n",
       "   'Number of steps': '12',\n",
       "   'How long did this take?': '8 minutes',\n",
       "   'Tools': '1. Web browser\\n2. Image recognition tools (to identify and parse a figure with three axes)',\n",
       "   'Number of tools': '2'},\n",
       "  {'Steps': '1. Search the web for “finding nemo main character”.\\n2. Note the results, which state that the main character is a clownfish.\\n3. Search the web for “usgs nonnative species database”.\\n4. Click result for the Nonindigenous Aquatic Species site.\\n5. Click “Marine Fishes”.\\n6. Click “Species List of Nonindigenous Marine Fish”.\\n7. Scroll through the list until I find the clown anenomefish, and click “Collection info”.\\n8. Note the place that a clown anenomefish was found, in Fred Howard Park at the Gulf of Mexico.\\n9. Search the web for “fred howard park florida zip code”.\\n10. Note the zip code, 34689. Since only one clownfish was found before the year 2020, this is the answer.',\n",
       "   'Number of steps': '10',\n",
       "   'How long did this take?': '5 minutes',\n",
       "   'Tools': '1. Search engine\\n2. Web browser',\n",
       "   'Number of tools': '2'},\n",
       "  {'Steps': '1. Find how many articles were published in Nature in 2020 by Googling \"articles submitted to nature 2020\"\\n2. Click through to Nature\\'s archive for 2020 and filter the results to only provide articles, not other types of publications: 1002\\n3. Find 4% of 1002 and round up: 40.08 > 41',\n",
       "   'Number of steps': '3',\n",
       "   'How long did this take?': '5 minutes',\n",
       "   'Tools': '1. search engine\\n2. calculator',\n",
       "   'Number of tools': '2'},\n",
       "  {'Steps': '1. Searched \"Unlambda syntax\" online (optional).\\n2. Opened https://en.wikipedia.org/wiki/Unlambda.\\n3. Note that the hello world program is very similar in syntax to the code in this question.\\n4. Go to the source referenced by the hello world program.\\n5. From the referenced source, read what the components of the program do to understand that each period needs a backtick after the initial `r.\\n6. Observe that in the given code, there are 12 periods but only 11 backticks after the initial `r, so the missing character is a backtick.',\n",
       "   'Number of steps': '6',\n",
       "   'How long did this take?': '15 minutes',\n",
       "   'Tools': '1. Web browser\\n2. Search engine\\n3. Unlambda compiler (optional)',\n",
       "   'Number of tools': '3'},\n",
       "  {'Steps': '1. Googled Eliud Kipchoge marathon pace to find 4min 37sec/mile\\n2. Converted into fractions of hours.\\n3. Found moon periapsis in miles (225,623 miles).\\n4. Multiplied the two to find the number of hours and rounded to the nearest 100 hours.',\n",
       "   'Number of steps': '4',\n",
       "   'How long did this take?': '20 Minutes',\n",
       "   'Tools': '1. A web browser.\\n2. A search engine.\\n3. A calculator.',\n",
       "   'Number of tools': '3'},\n",
       "  {'Steps': '1. I did a search for Mercedes Sosa\\n2. I went to the Wikipedia page for her\\n3. I scrolled down to \"Studio albums\"\\n4. I counted the ones between 2000 and 2009',\n",
       "   'Number of steps': '4',\n",
       "   'How long did this take?': '5 minutes',\n",
       "   'Tools': '1. web browser\\n2. google search',\n",
       "   'Number of tools': '2'},\n",
       "  {'Steps': '1. Use search engine to search for \"British Museum search collection\" and navigate to the British Museum\\'s collection search webpage.\\n2. Select \"Museum number\" as search field and \"2012,5015.17\" in text box, then run search.\\n3. Open the page for the single result and note that the description says that this is the shell of an individual of the Nassa gibbosula species.\\n4. Use search engine to search for \"Nassa gibbosula\".\\n5. Note that according to the search result from the World Register of Marine Species website, Nassa gibbosula is not an accepted species name.\\n6. Open the page for Nassa gibbosula on the World Register of Marine Species website.\\n7. Scan the page and note that the accepted species name is Tritia gibbosula.\\n8. Use search engine to search for \"Science Advances 2021 Tritia gibbosula\".\\n9. Find that the top result is an article from 2021 in Science Advances titled \"Early Middle Stone Age personal ornaments from Bizmoune Cave, Essaouira, Morocco\".\\n10. Scan abstract and note that the article discusses beads made from Tritia gibbosula shells that date to at least 142 thousand years ago, giving a final answer of 142.',\n",
       "   'Number of steps': '10',\n",
       "   'How long did this take?': '12 minutes',\n",
       "   'Tools': '1. Web browser\\n2. Search engine',\n",
       "   'Number of tools': '2'},\n",
       "  {'Steps': '1. Searched \"numpy github\" on Google search.\\n2. Opened the NumPy GitHub page.\\n3. Clicked \"Issues\" in the repo tabs.\\n4. Clicked \"Closed\" on the filter bar.\\n5. Set the filter to the \"numpy.polynomial\" label.\\n6. Set the filter to the \"06 - Regression\" label.\\n7. Opened the oldest Regression post.\\n8. Scrolled down to find when the Regression label was added (Apr 15, 2018).\\n9. Converted to MM/DD/YY (04/15/18).',\n",
       "   'Number of steps': '9',\n",
       "   'How long did this take?': '10 minutes',\n",
       "   'Tools': '1. Web browser\\n2. Search engine',\n",
       "   'Number of tools': '2'},\n",
       "  {'Steps': 'Step 1: Evaluate the problem statement provided in my user\\'s prompt\\nStep 2: Consider the probability of any ball on the platform earning the prize.\\nStep 3: Evaluate the ball in position one. The probability of it earning the prize, P1, is 1/3\\nStep 4: Using a calculator, evaluate the ball in position two. The probability of it earning the prize, P2, is the difference between 1 and the product of the complementary probabilities for each trial\\nP2 = 1 - (2/3)(2/3)\\nP2 = 5/9\\nStep 5: Using a calculator, evaluate the ball in position three. The probability of it earning the prize, P3, is the difference between 1 and the product of the complementary probabilities for each trial\\nP3 = 1 - (2/3)(2/3)(2/3)\\nP3 = 19/27\\nStep 6: Consider the possible outcomes of numbers higher than 3.\\nStep 7: For each trial, either 1 or 2 balls from the ramp will advance to the platform. For any given selection, there is a 50% chance that the ball advances to position 2 or position 3.\\nStep 8: As position three holds the highest chance of earning the prize, select the only ball known to occupy position three with certainty, ball 3.\\nStep 9: Report the correct answer to my user, \"3\"',\n",
       "   'Number of steps': '9',\n",
       "   'How long did this take?': '1 minute',\n",
       "   'Tools': 'None',\n",
       "   'Number of tools': '0'},\n",
       "  {'Steps': '1. Searched \"July 2, 1959 United States standards for grades of processed fruits, vegetables, and certain other products\" on Google.\\n2. Opened https://upload.wikimedia.org/wikipedia/commons/0/06/United_States_standards_for_grades_of_processed_fruits%2C_vegetables%2C_and_certain_other_products_%28as_of_July_2%2C_1959%29_%28IA_unitedstatesstan14unit_4%29.pdf.\\n3. Scrolled to the \"DRIED or DEHYDRATED\" section.\\n4. Opened a new tab and searched \"united states standards for grades of dehydrated apples\".\\n5. Opened https://www.ams.usda.gov/grades-standards/dehydrated-apples-grades-and-standards.\\n6. Opened the \"U.S. Grade Standards for Dehydrated Apples (pdf)\" PDF.\\n7. Checked the date against the 1959 standards.\\n8. Repeated steps 4-7 for all dehydrated items in the \"DRIED or DEHYDRATED\" section:\\n9. Grapefruit Juice, updated (running tally: 2/2)\\n10. Orange Juice, updated (running tally: 3/3)\\n11. Found all versions of the dehydrated items in Frozen or Chilled, except those marked Chilled: Apples; Grapefruit Juice, Concentrated; Grapefruit Juice and Orange Juice, Concentrated, Blended; Orange Juice, Concentrated\\n12. Repeated steps 4-7 all those versions:\\n13. Apples, not updated (running tally: 3/4)\\n14. Grapefruit Juice, Concentrated, updated (running tally: 4/5)\\n15. Grapefruit Juice and Orange Juice, Concentrated, Blended, updated (running tally: 5/6)\\n16. Orange Juice, Concentrated, updated (running tally: 6/7)\\n17. Calculated the percentage (6 / 7 * 100% = 85.7%).\\n18. Rounded to the nearest percent (86%).',\n",
       "   'Number of steps': '14',\n",
       "   'How long did this take?': '20 minutes',\n",
       "   'Tools': '1. Web browser\\n2. Search engine\\n3. PDF access\\n4. Calculator',\n",
       "   'Number of tools': '4'}]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_validation_data[range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3bbe152",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfiltered_test_data\u001b[49m.select(\u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m)):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(example[\u001b[33m\"\u001b[39m\u001b[33mfile_path\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'filtered_test_data' is not defined"
     ]
    }
   ],
   "source": [
    "for example in filtered_test_data.select(range(10)):\n",
    "    print(example[\"file_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "627b9b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 165/165 [00:00<00:00, 10722.63 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['task_id', 'Question', 'Level', 'Final answer', 'file_name', 'file_path', 'Annotator Metadata'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_test_data = validation_data.filter(lambda example: example[\"file_name\"].endswith(\".zip\"))\n",
    "\n",
    "filtered_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4047eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
